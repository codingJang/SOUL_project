Failure # 1 (occurred at 2023-11-16_17-36-30)
The actor died because of an error raised in its creation task, [36mray::PPO.__init__()[39m (pid=25338, ip=127.0.0.1, actor_id=cb68ffad7a03dd01930d6df801000000, repr=PPO)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 227, in _setup
    self.add_workers(
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 593, in add_workers
    raise result.get()
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/actor_manager.py", line 481, in __fetch_result
    result = ray.get(r)
ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=25359, ip=127.0.0.1, actor_id=b4da6f4aac5fce9726e1393601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fccf10bdf10>)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py", line 166, in step
    obss, rews, terminateds, truncateds, infos = self.par_env.step(action_dict)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/supersuit/generic_wrappers/utils/shared_wrapper_util.py", line 130, in step
    observations, rewards, terminations, truncations, infos = super().step(actions)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base_parallel.py", line 48, in step
    res = self.env.step(actions)
  File "/Users/jang-yejun/Projects/VSCodeProjects/SOUL_project/Source/RLlibPractice/politics_environment.py", line 91, in step
    invite = np.insert(action["invite"], i, False)
  File "<__array_function__ internals>", line 180, in insert
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py", line 5280, in insert
    raise IndexError(f"index {obj} is out of bounds for axis {axis} "
IndexError: index 2 is out of bounds for axis 0 with size 1

The above exception was the direct cause of the following exception:

[36mray::RolloutWorker.__init__()[39m (pid=25359, ip=127.0.0.1, actor_id=b4da6f4aac5fce9726e1393601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fccf10bdf10>)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
    raise ValueError(
ValueError: Your environment (<ParallelPettingZooEnv instance>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
In particular, the `step()` method seems to be faulty.
Learn more about the most important changes here:
https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium

In order to fix this problem, do the following:

1) Run `pip install gymnasium` on your command line.
2) Change all your import statements in your code from
   `import gym` -> `import gymnasium as gym` OR
   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`

For your custom (single agent) gym.Env classes:
3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
     EnvCompatibility` wrapper class.
3.2) Alternatively to 3.1:
 - Change your `reset()` method to have the call signature 'def reset(self, *,
   seed=None, options=None)'
 - Return an additional info dict (empty dict should be fine) from your `reset()`
   method.
 - Return an additional `truncated` flag from your `step()` method (between `done` and
   `info`). This flag should indicate, whether the episode was terminated prematurely
   due to some time constraint or other kind of horizon setting.

For your custom RLlib `MultiAgentEnv` classes:
4.1) Either wrap your old MultiAgentEnv via the provided
     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
     MultiAgentEnvCompatibility` wrapper class.
4.2) Alternatively to 4.1:
 - Change your `reset()` method to have the call signature
   'def reset(self, *, seed=None, options=None)'
 - Return an additional per-agent info dict (empty dict should be fine) from your
   `reset()` method.
 - Rename `dones` into `terminateds` and only set this to True, if the episode is really
   done (as opposed to has been terminated prematurely due to some horizon/time-limit
   setting).
 - Return an additional `truncateds` per-agent dictionary flag from your `step()`
   method, including the `__all__` key (100% analogous to your `dones/terminateds`
   per-agent dict).
   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
   flag should indicate, whether the episode (for some agent or all agents) was
   terminated prematurely due to some time constraint or other kind of horizon setting.


During handling of the above exception, another exception occurred:

[36mray::RolloutWorker.__init__()[39m (pid=25359, ip=127.0.0.1, actor_id=b4da6f4aac5fce9726e1393601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fccf10bdf10>)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/rollout_worker.py", line 404, in __init__
    check_env(self.env, self.config)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 96, in check_env
    raise ValueError(
ValueError: Traceback (most recent call last):
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 363, in check_multiagent_environments
    results = env.step(sampled_action)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py", line 166, in step
    obss, rews, terminateds, truncateds, infos = self.par_env.step(action_dict)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/supersuit/generic_wrappers/utils/shared_wrapper_util.py", line 130, in step
    observations, rewards, terminations, truncations, infos = super().step(actions)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base_parallel.py", line 48, in step
    res = self.env.step(actions)
  File "/Users/jang-yejun/Projects/VSCodeProjects/SOUL_project/Source/RLlibPractice/politics_environment.py", line 91, in step
    invite = np.insert(action["invite"], i, False)
  File "<__array_function__ internals>", line 180, in insert
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py", line 5280, in insert
    raise IndexError(f"index {obj} is out of bounds for axis {axis} "
IndexError: index 2 is out of bounds for axis 0 with size 1

The above exception was the direct cause of the following exception:

[36mray::RolloutWorker.__init__()[39m (pid=25359, ip=127.0.0.1, actor_id=b4da6f4aac5fce9726e1393601000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fccf10bdf10>)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
    raise ValueError(
ValueError: Your environment (<ParallelPettingZooEnv instance>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
In particular, the `step()` method seems to be faulty.
Learn more about the most important changes here:
https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium

In order to fix this problem, do the following:

1) Run `pip install gymnasium` on your command line.
2) Change all your import statements in your code from
   `import gym` -> `import gymnasium as gym` OR
   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`

For your custom (single agent) gym.Env classes:
3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
     EnvCompatibility` wrapper class.
3.2) Alternatively to 3.1:
 - Change your `reset()` method to have the call signature 'def reset(self, *,
   seed=None, options=None)'
 - Return an additional info dict (empty dict should be fine) from your `reset()`
   method.
 - Return an additional `truncated` flag from your `step()` method (between `done` and
   `info`). This flag should indicate, whether the episode was terminated prematurely
   due to some time constraint or other kind of horizon setting.

For your custom RLlib `MultiAgentEnv` classes:
4.1) Either wrap your old MultiAgentEnv via the provided
     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
     MultiAgentEnvCompatibility` wrapper class.
4.2) Alternatively to 4.1:
 - Change your `reset()` method to have the call signature
   'def reset(self, *, seed=None, options=None)'
 - Return an additional per-agent info dict (empty dict should be fine) from your
   `reset()` method.
 - Rename `dones` into `terminateds` and only set this to True, if the episode is really
   done (as opposed to has been terminated prematurely due to some horizon/time-limit
   setting).
 - Return an additional `truncateds` per-agent dictionary flag from your `step()`
   method, including the `__all__` key (100% analogous to your `dones/terminateds`
   per-agent dict).
   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
   flag should indicate, whether the episode (for some agent or all agents) was
   terminated prematurely due to some time constraint or other kind of horizon setting.


The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).

During handling of the above exception, another exception occurred:

[36mray::PPO.__init__()[39m (pid=25338, ip=127.0.0.1, actor_id=cb68ffad7a03dd01930d6df801000000, repr=PPO)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py", line 517, in __init__
    super().__init__(
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py", line 161, in __init__
    self.setup(copy.deepcopy(self.config))
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py", line 639, in setup
    self.workers = WorkerSet(
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/evaluation/worker_set.py", line 179, in __init__
    raise e.args[0].args[2]
ValueError: Traceback (most recent call last):
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 363, in check_multiagent_environments
    results = env.step(sampled_action)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/env/wrappers/pettingzoo_env.py", line 166, in step
    obss, rews, terminateds, truncateds, infos = self.par_env.step(action_dict)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/supersuit/generic_wrappers/utils/shared_wrapper_util.py", line 130, in step
    observations, rewards, terminations, truncations, infos = super().step(actions)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/pettingzoo/utils/wrappers/base_parallel.py", line 48, in step
    res = self.env.step(actions)
  File "/Users/jang-yejun/Projects/VSCodeProjects/SOUL_project/Source/RLlibPractice/politics_environment.py", line 91, in step
    invite = np.insert(action["invite"], i, False)
  File "<__array_function__ internals>", line 180, in insert
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py", line 5280, in insert
    raise IndexError(f"index {obj} is out of bounds for axis {axis} "
IndexError: index 2 is out of bounds for axis 0 with size 1

The above exception was the direct cause of the following exception:

[36mray::PPO.__init__()[39m (pid=25338, ip=127.0.0.1, actor_id=cb68ffad7a03dd01930d6df801000000, repr=PPO)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 81, in check_env
    check_multiagent_environments(env)
  File "/Users/jang-yejun/opt/anaconda3/lib/python3.9/site-packages/ray/rllib/utils/pre_checks/env.py", line 368, in check_multiagent_environments
    raise ValueError(
ValueError: Your environment (<ParallelPettingZooEnv instance>) does not abide to the new gymnasium-style API!
From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.
In particular, the `step()` method seems to be faulty.
Learn more about the most important changes here:
https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium

In order to fix this problem, do the following:

1) Run `pip install gymnasium` on your command line.
2) Change all your import statements in your code from
   `import gym` -> `import gymnasium as gym` OR
   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`

For your custom (single agent) gym.Env classes:
3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import
     EnvCompatibility` wrapper class.
3.2) Alternatively to 3.1:
 - Change your `reset()` method to have the call signature 'def reset(self, *,
   seed=None, options=None)'
 - Return an additional info dict (empty dict should be fine) from your `reset()`
   method.
 - Return an additional `truncated` flag from your `step()` method (between `done` and
   `info`). This flag should indicate, whether the episode was terminated prematurely
   due to some time constraint or other kind of horizon setting.

For your custom RLlib `MultiAgentEnv` classes:
4.1) Either wrap your old MultiAgentEnv via the provided
     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import
     MultiAgentEnvCompatibility` wrapper class.
4.2) Alternatively to 4.1:
 - Change your `reset()` method to have the call signature
   'def reset(self, *, seed=None, options=None)'
 - Return an additional per-agent info dict (empty dict should be fine) from your
   `reset()` method.
 - Rename `dones` into `terminateds` and only set this to True, if the episode is really
   done (as opposed to has been terminated prematurely due to some horizon/time-limit
   setting).
 - Return an additional `truncateds` per-agent dictionary flag from your `step()`
   method, including the `__all__` key (100% analogous to your `dones/terminateds`
   per-agent dict).
   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This
   flag should indicate, whether the episode (for some agent or all agents) was
   terminated prematurely due to some time constraint or other kind of horizon setting.


The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior via calling `config.environment(disable_env_checking=True)`. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([your env]).
